{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d8294-3328-4e07-ad16-8a03e9bbfdb9",
   "metadata": {},
   "source": [
    "# 欢迎来到您的第一个作业！\n",
    "说明如下。请尝试一下，如果遇到困难，可以查看解决方案文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9fa1fc-eac5-4d1d-9be4-541b3f2b3458",
   "metadata": {},
   "source": [
    "# 作业练习\n",
    "\n",
    "升级 day 1 项目，使用通过 Ollama 在本地运行的开源模型而不是 OpenAI 来总结网页\n",
    "\n",
    "如果您不想使用付费 API，可以使用此技术进行所有后续项目。\n",
    "\n",
    "**优点：**\n",
    "1. 无 API 费用 - 开源\n",
    "2. 数据不会离开您的电脑\n",
    "\n",
    "**缺点：**\n",
    "1. 性能显著低于前沿模型\n",
    "\n",
    "## Ollama 安装回顾\n",
    "\n",
    "只需访问 [ollama.com](https://ollama.com) 并安装即可！\n",
    "\n",
    "安装完成后，ollama 服务器应该已经在本地运行。\n",
    "如果您访问：\n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "您应该会看到消息 `Ollama is running`。\n",
    "\n",
    "如果不是，请打开一个新的终端（Mac）或 Powershell（Windows）并输入 `ollama serve`\n",
    "然后在另一个终端（Mac）或 Powershell（Windows）中，输入 `ollama pull llama3.2`\n",
    "然后再次尝试 [http://localhost:11434/](http://localhost:11434/)。\n",
    "\n",
    "如果 Ollama 在您的机器上运行缓慢，请尝试使用 `llama3.2:1b` 作为替代方案。从终端或 Powershell 运行 `ollama pull llama3.2:1b`，并将下面的代码从 `MODEL = \"llama3.2\"` 更改为 `MODEL = \"llama3.2:1b\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ddd15d-a3c5-4f4e-a678-873f56162724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://127.0.0.1:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac0a679-599c-441f-9bf2-ddc73d35b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用与 OpenAI 相同的格式创建消息列表\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"描述一下生成式 AI 的一些商业应用\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9c624-14f0-4945-a719-8ddb64f66f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ff514-e8bd-4985-a572-2ea28bb4fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让我们确保模型已加载\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9f644-522d-4e05-a691-56e7658c0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果出于任何原因这不起作用，请尝试以下单元格中的 2 个版本\n",
    "# 并仔细检查本实验顶部“Ollama 安装回顾”中的说明\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a021f13-d6a1-4b96-8e18-4eae49d876fe",
   "metadata": {},
   "source": [
    "# 介绍 ollama 包\n",
    "\n",
    "现在我们将做同样的事情，但使用优雅的 ollama python 包而不是直接的 HTTP 调用。\n",
    "\n",
    "在底层，它与上面一样调用运行在 localhost:11434 的 ollama 服务器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745b9c4-57dc-4867-9180-61fa5db55eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4704e10-f5fb-4c15-a935-f046c06fb13d",
   "metadata": {},
   "source": [
    "## 另一种方法 - 使用 OpenAI python 库连接到 Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23057e00-b6fc-4678-93a9-6b31cb704bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实际上还有另一种方法，有些人可能更喜欢\n",
    "# 您可以使用 OpenAI 客户端 python 库调用 Ollama：\n",
    "\n",
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://127.0.0.1:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e22da-b891-41f6-9ac9-bd0c0a5f4f44",
   "metadata": {},
   "source": [
    "## 您是否对为什么这能起作用感到困惑？\n",
    "\n",
    "这看起来很奇怪，对吧？我们刚刚使用了 OpenAI 的代码来调用 Ollama？？这是怎么回事？！\n",
    "\n",
    "事情是这样的：\n",
    "\n",
    "python 类 `OpenAI` 只是 OpenAI 工程师编写的用于通过互联网调用端点的代码。\n",
    "\n",
    "当您调用 `openai.chat.completions.create()` 时，这段 python 代码只是向以下 url 发起一个网络请求：\"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "像这样的代码被称为“客户端库” - 它只是运行在您机器上的包装代码，用于发起网络请求。GPT 的实际能力运行在 OpenAI 的云端，位于此 API 之后，而不是在您的计算机上！\n",
    "\n",
    "OpenAI 非常流行，许多其他 AI 提供商也提供了相同的网络端点，因此您可以使用相同的方法。\n",
    "\n",
    "因此 Ollama 在您的本地设备上运行着一个端点，地址是 http://localhost:11434/v1/chat/completions\n",
    "在第二周，我们将发现许多其他提供商也这样做，包括 Gemini 和 DeepSeek。\n",
    "\n",
    "然后 OpenAI 的团队有了一个很棒的主意：他们可以扩展他们的客户端库，以便您可以指定不同的“基本 url”，并使用他们的库来调用任何兼容的 API。\n",
    "\n",
    "就是这样！\n",
    "\n",
    "所以当您说：`ollama_via_openai = OpenAI(base_url='http://127.0.0.1:11434/v1', api_key='ollama')`\n",
    "那么这将发起相同的端点调用，但目标是 Ollama 而不是 OpenAI。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d1de3-e2ac-46ff-a302-3b4ba38c4c90",
   "metadata": {},
   "source": [
    "## 也尝试一下惊人的推理模型 DeepSeek\n",
    "\n",
    "这里我们使用 DeepSeek-reasoner 的 1.5B 精简版本。\n",
    "这实际上是 Qwen 的一个 1.5B 变体，已使用 Deepseek R1 生成的合成数据进行了微调。\n",
    "其他大小的 DeepSeek [在这里](https://ollama.com/library/deepseek-r1)，一直到完整的 671B 参数版本，这将占用您 404GB 的硬盘空间，对于大多数人来说太大了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9eb44e-fe5b-47aa-b719-0bb63669ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3d554b-e00d-4c08-9300-45e073950a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这可能需要几分钟才能运行！然后您应该会在 <think> 标签内看到一个引人入胜的“思考”轨迹，后面是一些不错的定义\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"请给出 LLM 背后一些核心概念的定义：神经网络、注意力和 Transformer\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622d9bb-5c68-4d4e-9ca4-b492c751f898",
   "metadata": {},
   "source": [
    "# 现在是您的练习\n",
    "\n",
    "将 day1 的代码整合到这里，构建一个使用本地运行的 Llama 3.2 而不是 OpenAI 的网站摘要工具；使用上述任一方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de38216-6d1c-48c4-877b-86d403f4e0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
