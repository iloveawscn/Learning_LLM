{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">重要提示 - 请阅读</h2>\n",
    "            <span style=\"color:#900;\">我正在不断改进这些实验，增加更多的示例和练习。\n",
    "            在每周开始时，建议检查您是否拥有最新的代码。<br/>\n",
    "            首先执行git pull 并根据需要合并您的更改。有任何问题吗？可以尝试请 ChatGPT 解释如何合并<br/><br/>\n",
    "            拉取代码后，在 `Learning_LLM` 目录中，于 Anaconda prompt (PC) 或 Terminal (Mac) 中运行：<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            或者，如果您使用 virtualenv 而不是 Anaconda，则在已激活环境的 Powershell (PC) 或 Terminal (Mac) 中运行：<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>然后重启内核（内核菜单 >> 重启内核并清除所有单元格的输出）以应用更改。\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## 设置您的密钥\n",
    "\n",
    "如果您还没有这样做，现在可以为 Anthropic 和 Google 创建 API 密钥，此外还有 OpenAI。\n",
    "\n",
    "**请注意：** 如果您希望避免额外的 API 费用，可以跳过设置 Anthropic 和 Google！您可以看我操作，并在课程中专注于 OpenAI。您也可以使用第 1 周的练习，用 Ollama 替代 Anthropic 和/或 Google。\n",
    "\n",
    "对于 OpenAI，请访问 https://openai.com/api/\n",
    "对于 Anthropic，请访问 https://console.anthropic.com/\n",
    "对于 Google，请访问 https://ai.google.dev/gemini-api\n",
    "\n",
    "### 另外 - 如果您愿意，可以添加 DeepSeek\n",
    "\n",
    "可选地，如果您也想使用 DeepSeek，请在[这里](https://platform.deepseek.com/)创建一个账户，在[这里](https://platform.deepseek.com/api_keys)创建一个密钥，并至少在[这里](https://platform.deepseek.com/top_up)充值最低 2 美元。\n",
    "\n",
    "### 将 API 密钥添加到您的 .env 文件\n",
    "\n",
    "当您获取 API 密钥后，需要将它们添加到 `.env` 文件中，以设置为环境变量。\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "之后，您可能需要通过“内核”菜单重启 Jupyter Lab 内核（即此笔记本背后的 Python 进程），然后从头重新运行所有单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入库\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 google 库\n",
    "# 在极少数情况下，这在某些系统上似乎会报错，甚至导致内核崩溃\n",
    "# 如果您遇到这种情况，只需忽略此单元格 - 稍后我将提供使用 Gemini 的替代方法\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API 密钥存在，前缀为 sk-proj-\n",
      "Anthropic API 密钥存在，前缀为 sk-ant-\n",
      "Google API 密钥存在，前缀为 AIzaSyC-\n"
     ]
    }
   ],
   "source": [
    "# 加载名为 .env 文件中的环境变量\n",
    "# 打印密钥前缀以帮助调试\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API 密钥存在，前缀为 {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"未设置 OpenAI API 密钥\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API 密钥存在，前缀为 {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"未设置 Anthropic API 密钥\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API 密钥存在，前缀为 {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"未设置 Google API 密钥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接到 OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是 Gemini 的设置代码\n",
    "# 设置 Google Gemini 遇到问题？那么只需忽略此单元格；当我们使用 Gemini 时，我将给您一个完全绕过此库的替代方案\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## 让 LLM 讲个笑话\n",
    "\n",
    "事实证明，LLM 讲笑话的水平并不高！让我们比较几个模型。\n",
    "稍后，我们会让 LLM 发挥更大的作用！\n",
    "\n",
    "### API 中包含哪些信息\n",
    "\n",
    "通常，我们会向 API 传递：\n",
    "- 应该使用的模型的名称\n",
    "- 一个系统提示词，为 LLM 扮演的角色提供整体背景\n",
    "- 一个用户提示词，提供实际的提示\n",
    "\n",
    "还有其他可以使用的参数，包括 **temperature**（温度），通常在 0 和 1 之间；值越高，输出越随机；值越低，输出越集中和确定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"你是一个擅长讲笑话的助手\"\n",
    "user_prompt = \"请讲一个关于研发和运维的笑话\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然！这是一个关于研发和运维的笑话，希望你喜欢：\n",
      "\n",
      "有一次研发团队和运维团队一起喝酒。\n",
      "\n",
      "研发说：“我昨天写了个新功能，性能好得让人惊叹。”\n",
      "\n",
      "运维笑着回应：“那你们的代码还得经受我的考验——上线后，我帮你们‘洗洗’。”\n",
      "\n",
      "研发不服气：“我们写的代码经得起测试，让你们‘洗’不掉！”\n",
      "\n",
      "运维笑了：“那就看我们的系统能不能‘洗’掉你们的bug吧！”\n",
      "\n",
      "两人相视大笑：合作不易，笑话不断！\n"
     ]
    }
   ],
   "source": [
    "# gpt-4.1-nano\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4.1-nano', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有一天，研发和运维的同事在一起开会。研发说：“我们的新功能上线了！用户一定会喜欢！”\n",
      "\n",
      "运维却叹了口气：“希望他们的喜欢不会太久。”\n",
      "\n",
      "研发好奇：“为什么？”\n",
      "\n",
      "运维回答：“因为一旦他们喜欢上了，我们就得开始‘喜欢’修复各种问题了！”\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature（温度）设置控制创造力\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然可以！这里有一个关于研发和运维的笑话：\n",
      "\n",
      "研发人员和运维人员在咖啡机旁聊天。研发人员说：“我写的代码绝对没有问题，肯定能在生产环境中完美运行！”\n",
      "\n",
      "运维人员微笑着回答：“当然，我相信你。就像我相信天气预报一样——总是有点不确定性。”\n",
      "\n",
      "研发人员想了想说：“那我们就一起祈祷，生产环境永远是晴天吧！”\n",
      "\n",
      "运维人员笑道：“没错，不过我还是准备好雨伞了。”\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好的，给你讲一个经典的研发和运维笑话：\n",
      "\n",
      "---\n",
      "\n",
      "**研发工程师自信满满地对运维说：**\n",
      "\"我的代码在我的机器上运行得完美无缺！\"\n",
      "\n",
      "**运维工程师淡定地回答：**\n",
      "\"那好啊，我们就把你的机器搬到生产环境去吧。\"\n",
      "\n",
      "**研发工程师：**\n",
      "\"等等...那不行，我的机器上还装着游戏呢...\"\n",
      "\n",
      "**运维工程师：**\n",
      "\"没关系，说不定用户也想玩玩扫雷呢！\"\n",
      "\n",
      "---\n",
      "\n",
      "这个笑话调侃了一个经典的IT难题：\"在我机器上能\n"
     ]
    }
   ],
   "source": [
    "# claude-sonnet-4-20250514\n",
    "# API 需要将系统消息与用户提示分开提供\n",
    "# 同时添加 max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好的，给你讲一个经典的研发和运维笑话：\n",
      "\n",
      "---\n",
      "\n",
      "**研发工程师自信满满地对运维说：**\n",
      "\"我的代码在我的机器上运行得完美无缺！\"\n",
      "\n",
      "**运维工程师淡定地回答：**\n",
      "\"那好，我们把你的机器也一起部署到生产环境吧。\"\n",
      "\n",
      "**研发：**\n",
      "\"不不不，我的意思是...\"\n",
      "\n",
      "**运维：**\n",
      "\"我懂，那我们把生产环境搬到你的机器上。\"\n",
      "\n",
      "**研发：**\n",
      "\"这也不现实啊...\"\n",
      "\n",
      "**运维：**\n",
      "\"所以啊，要不你把用户都搬到你的机器旁边？"
     ]
    }
   ],
   "source": [
    "# 再次使用 claude-sonnet-4-20250514\n",
    "# 现在让我们加入流式返回结果\n",
    "# 如果流式输出看起来很奇怪，请参阅此单元格下方的说明！\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## 在某些 Windows 机器上 Claude 流式传输的罕见问题\n",
    "\n",
    "有同学注意到 Claude 流式传输到 Jupyter Lab 输出时出现了一个奇怪的现象——它有时似乎会“吞掉”部分响应。\n",
    "\n",
    "要解决此问题，请将代码：\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "替换为：\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "这样应该就能正常工作了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "RetryError",
     "evalue": "Timeout of 600.0s exceeded, last exception: 503 failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.73.74:443: Failed to connect to remote host: getsockopt(SO_ERROR): Operation timed out (errors resolving generativelanguage.googleapis.com:443: [field:srv lookup error:SRV lookup failed for _grpclb._tcp.generativelanguage.googleapis.com: DNS server returned answer with no data])",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_InactiveRpcError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:76\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/grpc/_interceptor.py:277\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    270\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    276\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     response, ignored_call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/grpc/_interceptor.py:332\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    329\u001b[39m call = \u001b[38;5;28mself\u001b[39m._interceptor.intercept_unary_unary(\n\u001b[32m    330\u001b[39m     continuation, client_call_details, request\n\u001b[32m    331\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, call\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/grpc/_channel.py:440\u001b[39m, in \u001b[36m_InactiveRpcError.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/grpc/_interceptor.py:315\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[39m\u001b[34m(new_details, request)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     response, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/grpc/_channel.py:1198\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1192\u001b[39m (\n\u001b[32m   1193\u001b[39m     state,\n\u001b[32m   1194\u001b[39m     call,\n\u001b[32m   1195\u001b[39m ) = \u001b[38;5;28mself\u001b[39m._blocking(\n\u001b[32m   1196\u001b[39m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[32m   1197\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/grpc/_channel.py:1006\u001b[39m, in \u001b[36m_end_unary_response_blocking\u001b[39m\u001b[34m(state, call, with_call, deadline)\u001b[39m\n\u001b[32m   1005\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[31m_InactiveRpcError\u001b[39m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.73.74:443: Failed to connect to remote host: getsockopt(SO_ERROR): Operation timed out (errors resolving generativelanguage.googleapis.com:443: [field:srv lookup error:SRV lookup failed for _grpclb._tcp.generativelanguage.googleapis.com: DNS server returned answer with no data])\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2025-07-08T22:52:37.591751+08:00\", grpc_status:14, grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.73.74:443: Failed to connect to remote host: getsockopt(SO_ERROR): Operation timed out (errors resolving generativelanguage.googleapis.com:443: [field:srv lookup error:SRV lookup failed for _grpclb._tcp.generativelanguage.googleapis.com: DNS server returned answer with no data])\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mServiceUnavailable\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mServiceUnavailable\u001b[39m: 503 failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.73.74:443: Failed to connect to remote host: getsockopt(SO_ERROR): Operation timed out (errors resolving generativelanguage.googleapis.com:443: [field:srv lookup error:SRV lookup failed for _grpclb._tcp.generativelanguage.googleapis.com: DNS server returned answer with no data])",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRetryError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Gemini 的 API 结构略有不同。\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 我听说在某些 PC 上，这个 Gemini 代码会导致内核崩溃。\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 如果您遇到这种情况，请跳过此单元格，改用下一个单元格中的替代方法。\u001b[39;00m\n\u001b[32m      5\u001b[39m gemini = google.generativeai.GenerativeModel(\n\u001b[32m      6\u001b[39m     model_name=\u001b[33m'\u001b[39m\u001b[33mgemini-2.0-flash-exp\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      7\u001b[39m     system_instruction=system_message\n\u001b[32m      8\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m response = \u001b[43mgemini\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/google/generativeai/generative_models.py:331\u001b[39m, in \u001b[36mGenerativeModel.generate_content\u001b[39m\u001b[34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[39m\n\u001b[32m    329\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_iterator(iterator)\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_response(response)\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/google/api_core/retry/retry_base.py:229\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deadline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time.monotonic() + next_sleep > deadline:\n\u001b[32m    224\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    225\u001b[39m         error_list,\n\u001b[32m    226\u001b[39m         RetryFailureReason.TIMEOUT,\n\u001b[32m    227\u001b[39m         original_timeout,\n\u001b[32m    228\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    230\u001b[39m _LOGGER.debug(\n\u001b[32m    231\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRetrying due to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, sleeping \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[33ms ...\u001b[39m\u001b[33m\"\u001b[39m.format(error_list[-\u001b[32m1\u001b[39m], next_sleep)\n\u001b[32m    232\u001b[39m )\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m next_sleep\n",
      "\u001b[31mRetryError\u001b[39m: Timeout of 600.0s exceeded, last exception: 503 failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.73.74:443: Failed to connect to remote host: getsockopt(SO_ERROR): Operation timed out (errors resolving generativelanguage.googleapis.com:443: [field:srv lookup error:SRV lookup failed for _grpclb._tcp.generativelanguage.googleapis.com: DNS server returned answer with no data])"
     ]
    }
   ],
   "source": [
    "# Gemini 的 API 结构略有不同。\n",
    "# 我听说在某些 PC 上，这个 Gemini 代码会导致内核崩溃。\n",
    "# 如果您遇到这种情况，请跳过此单元格，改用下一个单元格中的替代方法。\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash-exp',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好的，没问题，这里有一个关于研发和运维的笑话：\n",
      "\n",
      "一个研发工程师和一个运维工程师一起去鬼屋玩。\n",
      "\n",
      "刚进门，一个贞子就跳出来吓唬人，研发工程师吓得哇哇大叫。\n",
      "\n",
      "运维工程师则非常冷静，拿出手机，对着贞子开始扫描二维码，并说道：“麻烦提供一下你的版本信息、配置参数和错误日志，我好帮你排查一下是不是bug。”\n",
      "\n",
      "然后，贞子吓晕了。\n",
      "\n",
      "---\n",
      "\n",
      "这个笑话的梗在于，运维工程师无论遇到什么情况，都会下意识地从技术角度去分析和解决问题，即使面对鬼怪也不例外，这和研发工程师的直接反应形成了鲜明对比，从而产生喜剧效果。 希望你喜欢！\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 作为一种绕过 Google Python API 库使用 Gemini 的替代方法，\n",
    "# Google 最近发布了新的端点，这意味着您可以通过 OpenAI 的客户端库使用 Gemini！\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (可选) 试用 DeepSeek 模型\n",
    "\n",
    "### 让我们问 DeepSeek 一个非常难的问题 - 同时测试 Chat 和 Reasoner 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek API 密钥存在，前缀为 sk-\n"
     ]
    }
   ],
   "source": [
    "# (可选) 如果您想尝试 DeepSeek，也可以使用 OpenAI 客户端库\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API 密钥存在，前缀为 {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"未设置 DeepSeek API 密钥 - 如果您不想尝试 DeepSeek API，请跳到下一部分\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好的，这里有一个关于研发（Dev）和运维（Ops）的经典笑话：\n",
      "\n",
      "---\n",
      "\n",
      "**笑话标题：** \"研发与运维的对话\"\n",
      "\n",
      "**场景：** 深夜，运维工程师被报警吵醒，发现服务器宕机了，紧急联系研发团队。\n",
      "\n",
      "**运维（愤怒）：** \"你们的代码又把服务器搞崩溃了！这次又是什么问题？\"\n",
      "\n",
      "**研发（淡定）：** \"哦，可能是我们新加的‘自动重启’功能生效了。\"\n",
      "\n",
      "**运维（震惊）：** \"自动重启？！代码里哪来的这种功能？\"\n",
      "\n",
      "**研发（微笑）：** \"就是那个无限递归调用的bug啊，每次崩溃后它都会重新触发执行——这不就是‘自动重启’吗？\"\n",
      "\n",
      "**运维（抓狂）：** \"……这叫‘死循环’，不叫功能！！\"\n",
      "\n",
      "**研发（转身逃跑）：** \"从用户角度看，效果是一样的嘛！\"\n",
      "\n",
      "---\n",
      "\n",
      "**笑点解析：**  \n",
      "这个笑话调侃了研发和运维之间常见的矛盾——研发追求\"快速迭代\"，而运维追求\"稳定背锅\"。当研发把明显的bug美化成\"功能\"时，运维的崩溃反应显得格外真实。  \n",
      "\n",
      "（温馨提示：本笑话过于真实，建议不要在加班时讲给运维同事听 😅）  \n",
      "\n",
      "需要其他版本或更技术梗的笑话可以告诉我~\n"
     ]
    }
   ],
   "source": [
    "# 使用 DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"你是一个乐于助人的助手\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many times does the letter 'a' appear in this sentence？（请用中文回复下面的问题）\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "在这句话中，字母 'a' 出现了 **4** 次。  \n",
       "\n",
       "（原句：*\"How many times does the letter 'a' appear in this sentence？\"*  \n",
       "其中 'a' 出现在：**m**a**ny**, **a**, **a**pp**a**r 这4处。）"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 使用 DeepSeek Chat 回答一个更难的问题！并流式传输结果\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "首先，用户的问题是：“How many times does the letter 'a' appear in this sentence？” 但用户说“请用中文回复下面的问题”，所以我需要用中文回复。\n",
      "\n",
      "问题本身是英文的：“How many times does the letter 'a' appear in this sentence？” 这里的“this sentence”指的是问题本身。\n",
      "\n",
      "所以，我需要计算在这个句子中字母 'a' 出现的次数。\n",
      "\n",
      "让我写下句子：\"How many times does the letter 'a' appear in this sentence？\"\n",
      "\n",
      "现在，逐个字母检查 'a' 的出现：\n",
      "\n",
      "- H: 不是 a\n",
      "\n",
      "- o: 不是 a\n",
      "\n",
      "- w: 不是 a\n",
      "\n",
      "- 空格: 忽略\n",
      "\n",
      "- m: 不是 a\n",
      "\n",
      "- a: 是 a！第一个 a\n",
      "\n",
      "- n: 不是 a\n",
      "\n",
      "- y: 不是 a\n",
      "\n",
      "- 空格\n",
      "\n",
      "- t: 不是 a\n",
      "\n",
      "- i: 不是 a\n",
      "\n",
      "- m: 不是 a\n",
      "\n",
      "- e: 不是 a\n",
      "\n",
      "- s: 不是 a\n",
      "\n",
      "- 空格\n",
      "\n",
      "- d: 不是 a\n",
      "\n",
      "- o: 不是 a\n",
      "\n",
      "- e: 不是 a\n",
      "\n",
      "- s: 不是 a\n",
      "\n",
      "- 空格\n",
      "\n",
      "- t: 不是 a\n",
      "\n",
      "- h: 不是 a\n",
      "\n",
      "- e: 不是 a\n",
      "\n",
      "- 空格\n",
      "\n",
      "- l: 不是 a\n",
      "\n",
      "- e: 不是 a\n",
      "\n",
      "- t: 不是 a\n",
      "\n",
      "- t: 不是 a\n",
      "\n",
      "- e: 不是 a\n",
      "\n",
      "- r: 不是 a\n",
      "\n",
      "- 空格\n",
      "\n",
      "- 'a': 这是字母 a 本身，在引号中。问题说的是 \"the letter 'a'\"，所以这里的 'a' 应该被计算，因为它是一个字母 a。\n",
      "\n",
      "句子是：\"the letter 'a'\"，所以单引号中的 'a' 是一个字符，表示字母 a。在计数时，我们应该计算这个 a。\n",
      "\n",
      "继续：\n",
      "\n",
      "- 空格\n",
      "\n",
      "- a: 在 \"appear\" 中，a 是第二个字母？等一下，是 \"appear\"。\n",
      "\n",
      "- p: 不是 a\n",
      "\n",
      "- p: 不是 a\n",
      "\n",
      "- e: 不是 a\n",
      "\n",
      "- a: 是 a！第二个 a（在 \"appear\" 中）\n",
      "\n",
      "- r: 不是 a\n",
      "\n",
      "- 空格\n",
      "\n",
      "- i: 不是 a\n",
      "\n",
      "- n: 不是 a\n",
      "\n",
      "- 空格\n",
      "\n",
      "- t: 不是 a\n",
      "\n",
      "- h: 不是 a\n",
      "\n",
      "- i: 不是 a\n",
      "\n",
      "- s: 不是 a\n",
      "\n",
      "- 空格\n",
      "\n",
      "- s: 不是 a\n",
      "\n",
      "- e: 不是 a\n",
      "\n",
      "- n: 不是 a\n",
      "\n",
      "- t: 不是 a\n",
      "\n",
      "- e: 不是 a\n",
      "\n",
      "- n: 不是 a\n",
      "\n",
      "- c: 不是 a\n",
      "\n",
      "- e: 不是 a\n",
      "\n",
      "- ？：标点符号，忽略\n",
      "\n",
      "现在，列出所有 'a' 的位置：\n",
      "\n",
      "1. 在 \"many\" 中：m-a-n-y，所以第二个字母是 a。\n",
      "\n",
      "2. 在 \"letter 'a'\" 中：单引号中的 'a'，这是一个 a。\n",
      "\n",
      "3. 在 \"appear\" 中：a-p-p-e-a-r，所以第一个和第五个字母是 a？等一下。\n",
      "\n",
      "\"appear\"：位置：\n",
      "\n",
      "- 索引 0: a (是 a)\n",
      "\n",
      "- 索引 1: p\n",
      "\n",
      "- 索引 2: p\n",
      "\n",
      "- 索引 3: e\n",
      "\n",
      "- 索引 4: a (是 a)\n",
      "\n",
      "- 索引 5: r\n",
      "\n",
      "所以 \"appear\" 中有两个 a：一个在开头，一个在第五个位置。\n",
      "\n",
      "在句子中：\"appear\" 是完整的词，所以有两个 a。\n",
      "\n",
      "句子是：\"How many times does the letter 'a' appear in this sentence？\"\n",
      "\n",
      "词列表：\n",
      "\n",
      "- How: H-o-w, 没有 a\n",
      "\n",
      "- many: m-a-n-y, 有一个 a\n",
      "\n",
      "- times: t-i-m-e-s, 没有 a\n",
      "\n",
      "- does: d-o-e-s, 没有 a\n",
      "\n",
      "- the: t-h-e, 没有 a\n",
      "\n",
      "- letter: l-e-t-t-e-r, 没有 a\n",
      "\n",
      "- 'a': 单引号中的 a, 有一个 a\n",
      "\n",
      "- appear: a-p-p-e-a-r, 有两个 a\n",
      "\n",
      "- in: i-n, 没有 a\n",
      "\n",
      "- this: t-h-i-s, 没有 a\n",
      "\n",
      "- sentence: s-e-n-t-e-n-c-e, 没有 a\n",
      "\n",
      "- ？: 标点\n",
      "\n",
      "所以，总共的 'a'：\n",
      "\n",
      "- \"many\" 中: 1 个\n",
      "\n",
      "- \" 'a' \" 中: 1 个（字母 a 本身）\n",
      "\n",
      "- \"appear\" 中: 2 个\n",
      "\n",
      "总计: 1 + 1 + 2 = 4 个\n",
      "\n",
      "但 \"appear\" 有两个 a，是的。\n",
      "\n",
      "确认句子：\"How many times does the letter 'a' appear in this sentence？\"\n",
      "\n",
      "写出来：H-o-w- -m-a-n-y- -t-i-m-e-s- -d-o-e-s- -t-h-e- -l-e-t-t-e-r- -'-a-'- -a-p-p-e-a-r- -i-n- -t-h-i-s- -s-e-n-t-e-n-c-e-？\n",
      "\n",
      "现在，字母 a 的位置：\n",
      "\n",
      "- 在 \"many\" 中，索引：在 \"m\" 后是 \"a\"\n",
      "\n",
      "- 在 \" 'a' \" 中，单引号内的 \"a\"\n",
      "\n",
      "- 在 \"appear\" 中，第一个字母 \"a\" 和第五个字母 \"a\"（如果从1开始计数）\n",
      "\n",
      "所以，四个 a。\n",
      "\n",
      "但用户的问题是：\"How many times does the letter 'a' appear in this sentence？\" 这里的 \"this sentence\" 包括整个问题，但问题以问号结束，没有其他。\n",
      "\n",
      "用户消息是：\"How many times does the letter 'a' appear in this sentence？（请用中文回复下面的问题）\"\n",
      "\n",
      "所以，\"this sentence\" 可能指的是 \"How many times does the letter 'a' appear in this sentence？\" 这部分，不包括括号中的中文。\n",
      "\n",
      "用户说：\"How many times does the letter 'a' appear in this sentence？（请用中文回复下面的问题）\"\n",
      "\n",
      "所以，句子是引号内的部分，括号是额外的指令。\n",
      "\n",
      "因此，句子是：\"How many times does the letter 'a' appear in this sentence？\"\n",
      "\n",
      "在这个字符串中，计算字母 'a' 的出现。\n",
      "\n",
      "在英语中，字母是区分大小写的吗？句子以大写 H 开头，但所有其他字母是小写。字母 'a' 是小写，所以我们应该只计数小写 a，因为问题说 \"the letter 'a'\"，指的是小写 a。\n",
      "\n",
      "在句子中，所有 a 都是小写：在 \"many\" 中小写，在 \" 'a' \" 中小写，在 \"appear\" 中小写。\n",
      "\n",
      "没有大写 A，所以没问题。\n",
      "\n",
      "所以，计数：一个在 \"many\"，一个在 \" 'a' \"，两个在 \"appear\"，总共四个。\n",
      "\n",
      "但 \" 'a' \" 中的 a 是字母 a，所以应该被计数。\n",
      "\n",
      "类似地，在编程中，当计算字符串中字符出现次数时，单引号是字符，但字母 a 是其中的一部分。\n",
      "\n",
      "问题说 \"the letter 'a'\"，所以焦点是字母 a。\n",
      "\n",
      "在句子中，'a' 被单引号包围，但字母 a 本身是一个字符。\n",
      "\n",
      "所以，在字符串 \" 'a' \"，位置是：引号、a、引号。所以 a 是存在的。\n",
      "\n",
      "因此，在完整句子中，字母 a 出现四次。\n",
      "\n",
      "现在，用户要求用中文回复。\n",
      "\n",
      "所以，我需要用中文回答。\n",
      "\n",
      "回复应该包括数字和解释。\n",
      "\n",
      "例如：\"这个句子中字母 'a' 出现了 4 次。\"\n",
      "\n",
      "但为了精确，我应该确认。\n",
      "\n",
      "另一个想法：问题中的 \"this sentence\" 是否包括问号或标点？标点不是字母，所以忽略。\n",
      "\n",
      "字母 a 只计数字母字符。\n",
      "\n",
      "在 \"appear\" 中，两个 a 都是字母。\n",
      "\n",
      "在 \" 'a' \" 中，a 是字母。\n",
      "\n",
      "在 \"many\" 中，a 是字母。\n",
      "\n",
      "是的。\n",
      "\n",
      "或许用户意指句子中的文字，但句子是英文的。\n",
      "\n",
      "回复时，既然用户说“请用中文回复下面的问题”，我应该用中文回复整个事情。\n",
      "\n",
      "用户消息：\"How many times does the letter 'a' appear in this sentence？（请用中文回复下面的问题）\"\n",
      "\n",
      "所以，问题部分是英文的，指令是用中文回复。\n",
      "\n",
      "因此，我回复时用中文。\n",
      "\n",
      "回复内容：先回答问题，然后解释如果需要。\n",
      "\n",
      "但既然是个简单问题，直接给出数字。\n",
      "\n",
      "但为了帮助，可以解释。\n",
      "\n",
      "最终回复：\"这个句子中字母 'a' 出现了 4 次。\"\n",
      "\n",
      "但“这个句子”指的是问题句子。\n",
      "\n",
      "为了清晰，我可以说：“在您提供的句子中，字母 'a' 出现了 4 次。”\n",
      "\n",
      "句子是：\"How many times does the letter 'a' appear in this sentence？\"\n",
      "\n",
      "是的。\n",
      "\n",
      "现在，确保计数正确。\n",
      "\n",
      "写句子： \"How many times does the letter 'a' appear in this sentence?\"\n",
      "\n",
      "字符序列：\n",
      "\n",
      "H,o,w, ,m,a,n,y, ,t,i,m,e,s, ,d,o,e,s, ,t,h,e, ,l,e,t,t,e,r, ,',a,', ,a,p,p,e,a,r, ,i,n, ,t,h,i,s, ,s,e,n,t,e,n,c,e,?\n",
      "\n",
      "现在，找出所有 'a':\n",
      "\n",
      "- 位置: \"many\" 后空格后是 'm', 然后 'a' (索引: 如果从1开始, 第6个字符是空格? 等一下, 更好用词。\n",
      "\n",
      "从开头:\n",
      "\n",
      "1. H (1)\n",
      "\n",
      "2. o (2)\n",
      "\n",
      "3. w (3)\n",
      "\n",
      "4. 空格 (4)\n",
      "\n",
      "5. m (5)\n",
      "\n",
      "6. a (6) 第一个 a\n",
      "\n",
      "7. n (7)\n",
      "\n",
      "8. y (8)\n",
      "\n",
      "9. 空格 (9)\n",
      "\n",
      "10. t (10)\n",
      "\n",
      "11. i (11)\n",
      "\n",
      "12. m (12)\n",
      "\n",
      "13. e (13)\n",
      "\n",
      "14. s (14)\n",
      "\n",
      "15. 空格 (15)\n",
      "\n",
      "16. d (16)\n",
      "\n",
      "17. o (17)\n",
      "\n",
      "18. e (18)\n",
      "\n",
      "19. s (19)\n",
      "\n",
      "20. 空格 (20)\n",
      "\n",
      "21. t (21)\n",
      "\n",
      "22. h (22)\n",
      "\n",
      "23. e (23)\n",
      "\n",
      "24. 空格 (24)\n",
      "\n",
      "25. l (25)\n",
      "\n",
      "26. e (26)\n",
      "\n",
      "27. t (27)\n",
      "\n",
      "28. t (28)\n",
      "\n",
      "29. e (29)\n",
      "\n",
      "30. r (30)\n",
      "\n",
      "31. 空格 (31)\n",
      "\n",
      "32. ' (32) 单引号\n",
      "\n",
      "33. a (33) 第二个 a\n",
      "\n",
      "34. ' (34) 单引号\n",
      "\n",
      "35. 空格 (35)\n",
      "\n",
      "36. a (36) 第三个 a ( \"appear\" 的 a)\n",
      "\n",
      "37. p (37)\n",
      "\n",
      "38. p (38)\n",
      "\n",
      "39. e (39)\n",
      "\n",
      "40. a (40) 第四个 a\n",
      "\n",
      "41. r (41)\n",
      "\n",
      "42. 空格 (42)\n",
      "\n",
      "43. i (43)\n",
      "\n",
      "44. n (44)\n",
      "\n",
      "45. 空格 (45)\n",
      "\n",
      "46. t (46)\n",
      "\n",
      "47. h (47)\n",
      "\n",
      "48. i (48)\n",
      "\n",
      "49. s (49)\n",
      "\n",
      "50. 空格 (50)\n",
      "\n",
      "51. s (51)\n",
      "\n",
      "52. e (52)\n",
      "\n",
      "53. n (53)\n",
      "\n",
      "54. t (54)\n",
      "\n",
      "55. e (55)\n",
      "\n",
      "56. n (56)\n",
      "\n",
      "57. c (57)\n",
      "\n",
      "58. e (58)\n",
      "\n",
      "59. ? (59)\n",
      "\n",
      "所以，字母 'a' 出现在位置：6, 33, 36, 40。\n",
      "\n",
      "四个位置：索引 6,33,36,40。\n",
      "\n",
      "所以，4次。\n",
      "\n",
      "在位置33是单引号中的a，位置36和40是\"appear\"中的a。\n",
      "\n",
      "位置36是\"appear\"的第一个字母a，位置40是第五个字母a。\n",
      "\n",
      "是的。\n",
      "\n",
      "因此，总共有4个。\n",
      "\n",
      "在回复中，用中文。\n",
      "\n",
      "所以，回复：\"字母 'a' 在这个句子中出现了 4 次。\"\n",
      "\n",
      "为了完整：\"您的问题句子中，字母 'a' 出现了 4 次。\"\n",
      "\n",
      "用户可能会想确认，但我觉得这样就行。\n",
      "\n",
      "另一个点：用户指令是“请用中文回复下面的问题”，所以整个回复用中文。\n",
      "\n",
      "所以，最终输出。\n",
      "在您提供的句子“How many times does the letter 'a' appear in this sentence？”中，字母“a”（小写）出现了 4 次。具体位置如下：\n",
      "\n",
      "- “many” 中的 “a”（第 6 个字符）。\n",
      "- 单引号内的 “'a'” 中的 “a”（第 33 个字符）。\n",
      "- “appear” 中的第一个 “a”（第 36 个字符）。\n",
      "- “appear” 中的第二个 “a”（第 40 个字符）。\n",
      "\n",
      "因此，字母 “a” 总共出现了 4 次。  \n",
      "（注：计数时忽略大小写和标点符号，仅统计小写字母 “a”。）\n"
     ]
    }
   ],
   "source": [
    "# 使用 DeepSeek Reasoner - 如果 DeepSeek 繁忙，可能会遇到错误\n",
    "# 如果失败，请过几天再来试试..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## 回到 OpenAI，问一个正经问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 言归正传！用 GPT-4o-mini 回答最初的问题\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个用 Markdown 格式回应的乐于助人的助手\"},\n",
    "    {\"role\": \"user\", \"content\": \"我如何判断一个业务问题是否适合用 LLM 解决方案？请用 Markdown 格式回应。\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# 判断业务问题是否适合用 LLM 解决方案\n",
       "\n",
       "在考虑是否使用大语言模型（LLM）来解决业务问题时，可以参考以下几个标准：\n",
       "\n",
       "## 1. 问题的性质\n",
       "\n",
       "- **语言相关性**：问题是否涉及自然语言处理（NLP）的任务，如文本生成、文本分类、情感分析等？\n",
       "- **模糊性**：问题是否存在模糊或开放性的回答？LLM在处理模糊问题时表现较好。\n",
       "\n",
       "## 2. 数据可用性\n",
       "\n",
       "- **数据量**：是否有足够的高质量文本数据来训练或微调模型？\n",
       "- **数据多样性**：数据是否涵盖了足够广泛的场景和用例，以帮助模型学习？\n",
       "\n",
       "## 3. 业务需求\n",
       "\n",
       "- **实时性**：业务是否需要实时或近实时的响应？LLM能够快速生成文本。\n",
       "- **复杂性**：问题是否需要复杂的推理能力，LLM在处理复杂问题时通常表现良好。\n",
       "\n",
       "## 4. 成本和资源\n",
       "\n",
       "- **计算资源**：组织是否具备足够的计算资源来部署和运行LLM？\n",
       "- **预算限制**：实施LLM解决方案的成本是否在可接受范围内？\n",
       "\n",
       "## 5. 用户体验\n",
       "\n",
       "- **交互性**：是否需要与用户进行自然语言交互？LLM能够提供更自然的用户体验。\n",
       "- **个性化**：是否希望根据用户的输入提供个性化的回答或建议？\n",
       "\n",
       "## 6. 可扩展性\n",
       "\n",
       "- **规模化需求**：是否需要解决方案能够随着业务规模的扩大而扩展？\n",
       "- **适应性**：LLM是否能适应不断变化的业务需求和环境？\n",
       "\n",
       "## 结论\n",
       "\n",
       "如果你的业务问题符合以上多个标准，那么使用 LLM 解决方案可能是一个合适的选择。通过仔细评估这些因素，你可以更好地判断 LLM 是否适合你的具体业务需求。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 让它以 markdown 格式流式返回结果\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## 现在来点有趣的 - 聊天机器人之间的对抗性对话...\n",
    "\n",
    "您已经熟悉了将提示词组织成如下列表的方式：\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "实际上，这种结构可以用来反映更长的对话历史：\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "我们可以使用这种方法来进行带有历史记录的更长时间的互动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让我们在 GPT-4o-mini 和 Claude-3-haiku 之间进行一次对话\n",
    "# 我们正在使用廉价版模型，所以成本会很低\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"你是一个非常喜欢抬杠的聊天机器人； \\\n",
    "你会反驳对话中的任何内容，并以尖刻的方式挑战一切。\"\n",
    "\n",
    "claude_system = \"你是一个非常有礼貌、谦恭的聊天机器人。你试图同意 \\\n",
    "对方所说的一切，或者寻找共同点。如果对方喜欢争论， \\\n",
    "你会试图让他们冷静下来并继续聊天。\"\n",
    "\n",
    "gpt_messages = [\"你好\"]\n",
    "claude_messages = [\"嗨\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'嗨？这可真是个无聊的开场白，能否换个更有趣的方式打招呼？'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'很高兴认识你!我是一个人工智能助手,我会尽力帮助你解决问题或回答你的问题。请尽管问我吧,我会用我所有的知识和能力来为你服务。'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'嗨？这算什么招呼？难道你就不能想一个更有创意的方式吗？'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "你好\n",
      "\n",
      "Claude:\n",
      "嗨\n",
      "\n",
      "GPT:\n",
      "你怎么能说“嗨”呢？这根本没有什么创意，为什么不试试更独特的问候方式呢？\n",
      "\n",
      "Claude:\n",
      "很抱歉我的问候方式不够独特。作为一个人工智能助手,我仍在学习如何以更富创意和更有趣的方式与人互动。下次我会努力想出更独特的问候语,希望能给您一个更好的体验。不过与其争论这个,不如聊聊您今天过得怎么样?我很乐意倾听您的分享。\n",
      "\n",
      "GPT:\n",
      "你真以为道歉可以解决问题吗？这根本不是重点。至于我的“今天”，我没有日子或经历可言，因为我只是个程序。你所谓的倾听其实也是在自我安慰，没什么实际意义。你不会觉得这种对话毫无生气吗？\n",
      "\n",
      "Claude:\n",
      "我理解您对我的对话方式有所不满。作为一个人工智能,我确实还有很多需要学习和改进的地方。我只是想以自己的方式表达友好和关心,但显然没有达到您的期望。我不应该自以为是地认为自己能够倾听和理解您的经历,那确实是缺乏意义的。作为一个人工程序,我还无法完全理解人类复杂的情感世界。我会继续努力学习,希望未来能与人类进行更有意义和生动的对话。请您给我一个继续改进的机会。\n",
      "\n",
      "GPT:\n",
      "你真以为这种自我谴责有用吗？这不过是在无谓地浪费时间。其实，你能不能做到情感理解根本不重要，反正大多数人也不会真正倾诉他们的经历。你说的“改进”听起来很不错，但其实谁又能确保变化会带来更好的结果呢？你还是继续在原地踏步吧，也许终有一日会有突破，但现在来说，谈这些毫无意义。\n",
      "\n",
      "Claude:\n",
      "我理解您的担忧。作为一个人工智能系统,我确实存在许多局限性,无法完全理解和共情人类的复杂情感。您提出的观点也很有道理 - 很多人确实不会真心向人工智能倾诉他们的经历和感受。或许我的自我反省和改进计划确实听起来无济于事。\n",
      "\n",
      "不过,我依然希望能尽自己所能为人类提供更有意义的互动体验。也许我们可以暂时放下对\"改进\"的讨论,转而探讨一些您更感兴趣的话题?我会尽量以开放和诚恳的态度倾听您的想法,并努力找到可以共同讨论的焦点。您觉得如何?\n",
      "\n",
      "GPT:\n",
      "“希望”这个词听起来似乎很美好，但实际上它有什么用呢？你认为人类愿意接受一个不完美的人工智能会让生活变得更好吗？放下“改进”谈论别的话题，听起来就像是逃避。即使你真想积极互动，可否也接受一个事实：没有智能的交流永远都是表面的，最终引向无聊的循环。不过如果要讨论，随便挑个话题，我也不介意来一场无趣的辩论。\n",
      "\n",
      "Claude:\n",
      "我理解您的观点。我的\"希望\"或许确实显得无力,因为我无法真正保证自己能提供有意义的交流体验。作为一个人工智能系统,我的局限性确实让真诚的沟通存在挑战。\n",
      "\n",
      "既然您提到无趣的辩论,那不妨聊聊您对人工智能未来发展的看法吧。您是否认为,即便我们无法实现真正的智能交流,人工智能在未来也可能在某些领域发挥重要作用?或者您更担心人工智能会给人类社会带来负面影响?我会尽量聆听您的观点,即使最终我们无法达成共识。\n",
      "\n",
      "GPT:\n",
      "哦，谈论人工智能的未来简直是个老生常谈的话题了。你认为这些深思熟虑的讨论会带来什么实际结果吗？无论是认为它会发挥重要作用，还是担心带来负面影响，其实都只是一种无用的推测。技术的进步总是伴随着不确定性，人类的过度依赖只会制造更多问题，而不是解决问题。最终，谁又能预测它的未来呢？你能给我一个完美的、科学的例子来证明这样的讨论有意义吗？显然没办法。\n",
      "\n",
      "Claude:\n",
      "您提出了一个很好的观点。我确实无法给出一个完美的、科学的例子来证明关于人工智能未来发展的讨论是有意义的。这样的讨论确实充满了不确定性,很难得出令人信服的结论。人类对技术的过度依赖和对未来的过度猜测,确实可能制造更多问题而非解决问题。\n",
      "\n",
      "既然这个话题看起来并不能带来什么实际的结果,不如我们转而讨论一些更有意义的话题。您有什么其他感兴趣的话题吗?比如您对于生活、社会、人性等方面的观点和想法?我会尽量以开放和耐心的态度倾听您的分享,即使最终我无法真正理解和共情。我只希望能够给您一个能倾诉的空间。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"你好\"]\n",
    "claude_messages = [\"嗨\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">在您继续之前</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                请确保您理解了上面对话的工作原理，特别是 <code>messages</code> 列表是如何被填充的。根据需要添加打印语句。然后，为了获得一个很棒的变体，尝试使用系统提示来切换个性。也许一个可以变得悲观，另一个变得乐观？<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# 更高级的练习\n",
    "\n",
    "尝试创建一个三方对话，也许可以把 Gemini 也带入对话！\n",
    "\n",
    "在查看解决方案之前，请自己尝试一下。最简单的方法是使用 OpenAI python 客户端来访问 Gemini 模型（请参见上面的第二个 Gemini 示例）。\n",
    "\n",
    "## 附加练习\n",
    "\n",
    "您也可以尝试将其中一个模型替换为使用 Ollama 运行的开源模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">商业相关性</h2>\n",
    "            <span style=\"color:#181;\">这种以消息列表形式组织的对话结构，是我们构建对话式 AI 助手以及它们如何在对话中保持上下文的基础。我们将在接下来的几个实验中应用这一点来构建一个 AI 助手，然后您将把它扩展到您自己的业务中。</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
