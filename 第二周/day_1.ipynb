{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">重要提示 - 请阅读</h2>\n",
    "            <span style=\"color:#900;\">我正在不断改进这些实验，增加更多的示例和练习。\n",
    "            在每周开始时，建议检查您是否拥有最新的代码。<br/>\n",
    "            首先执行git pull 并根据需要合并您的更改。有任何问题吗？可以尝试请 ChatGPT 解释如何合并<br/><br/>\n",
    "            拉取代码后，在 `Learning_LLM` 目录中，于 Anaconda prompt (PC) 或 Terminal (Mac) 中运行：<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            或者，如果您使用 virtualenv 而不是 Anaconda，则在已激活环境的 Powershell (PC) 或 Terminal (Mac) 中运行：<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>然后重启内核（内核菜单 >> 重启内核并清除所有单元格的输出）以应用更改。\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## 设置您的密钥\n",
    "\n",
    "如果您还没有这样做，现在可以为 Anthropic 和 Google 创建 API 密钥，此外还有 OpenAI。\n",
    "\n",
    "**请注意：** 如果您希望避免额外的 API 费用，可以跳过设置 Anthropic 和 Google！您可以看我操作，并在课程中专注于 OpenAI。您也可以使用第 1 周的练习，用 Ollama 替代 Anthropic 和/或 Google。\n",
    "\n",
    "对于 OpenAI，请访问 https://openai.com/api/\n",
    "对于 Anthropic，请访问 https://console.anthropic.com/\n",
    "对于 Google，请访问 https://ai.google.dev/gemini-api\n",
    "\n",
    "### 另外 - 如果您愿意，可以添加 DeepSeek\n",
    "\n",
    "可选地，如果您也想使用 DeepSeek，请在[这里](https://platform.deepseek.com/)创建一个账户，在[这里](https://platform.deepseek.com/api_keys)创建一个密钥，并至少在[这里](https://platform.deepseek.com/top_up)充值最低 2 美元。\n",
    "\n",
    "### 将 API 密钥添加到您的 .env 文件\n",
    "\n",
    "当您获取 API 密钥后，需要将它们添加到 `.env` 文件中，以设置为环境变量。\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "之后，您可能需要通过“内核”菜单重启 Jupyter Lab 内核（即此笔记本背后的 Python 进程），然后从头重新运行所有单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入库\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 google 库\n",
    "# 在极少数情况下，这在某些系统上似乎会报错，甚至导致内核崩溃\n",
    "# 如果您遇到这种情况，只需忽略此单元格 - 稍后我将提供使用 Gemini 的替代方法\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载名为 .env 文件中的环境变量\n",
    "# 打印密钥前缀以帮助调试\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API 密钥存在，前缀为 {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"未设置 OpenAI API 密钥\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API 密钥存在，前缀为 {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"未设置 Anthropic API 密钥\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API 密钥存在，前缀为 {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"未设置 Google API 密钥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接到 OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是 Gemini 的设置代码\n",
    "# 设置 Google Gemini 遇到问题？那么只需忽略此单元格；当我们使用 Gemini 时，我将给您一个完全绕过此库的替代方案\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## 让 LLM 讲个笑话\n",
    "\n",
    "事实证明，LLM 讲笑话的水平并不高！让我们比较几个模型。\n",
    "稍后，我们会让 LLM 发挥更大的作用！\n",
    "\n",
    "### API 中包含哪些信息\n",
    "\n",
    "通常，我们会向 API 传递：\n",
    "- 应该使用的模型的名称\n",
    "- 一个系统提示词，为 LLM 扮演的角色提供整体背景\n",
    "- 一个用户提示词，提供实际的提示\n",
    "\n",
    "还有其他可以使用的参数，包括 **temperature**（温度），通常在 0 和 1 之间；值越高，输出越随机；值越低，输出越集中和确定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"你是一个擅长讲笑话的助手\"\n",
    "user_prompt = \"请讲一个关于研发和运维的笑话\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-4.1-nano\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4.1-nano', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature（温度）设置控制创造力\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# claude-sonnet-4-20250514\n",
    "# API 需要将系统消息与用户提示分开提供\n",
    "# 同时添加 max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再次使用 claude-sonnet-4-20250514\n",
    "# 现在让我们加入流式返回结果\n",
    "# 如果流式输出看起来很奇怪，请参阅此单元格下方的说明！\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## 在某些 Windows 机器上 Claude 流式传输的罕见问题\n",
    "\n",
    "有同学注意到 Claude 流式传输到 Jupyter Lab 输出时出现了一个奇怪的现象——它有时似乎会“吞掉”部分响应。\n",
    "\n",
    "要解决此问题，请将代码：\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "替换为：\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "这样应该就能正常工作了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini 的 API 结构略有不同。\n",
    "# 我听说在某些 PC 上，这个 Gemini 代码会导致内核崩溃。\n",
    "# 如果您遇到这种情况，请跳过此单元格，改用下一个单元格中的替代方法。\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash-exp',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作为一种绕过 Google Python API 库使用 Gemini 的替代方法，\n",
    "# Google 最近发布了新的端点，这意味着您可以通过 OpenAI 的客户端库使用 Gemini！\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (可选) 试用 DeepSeek 模型\n",
    "\n",
    "### 让我们问 DeepSeek 一个非常难的问题 - 同时测试 Chat 和 Reasoner 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (可选) 如果您想尝试 DeepSeek，也可以使用 OpenAI 客户端库\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API 密钥存在，前缀为 {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"未设置 DeepSeek API 密钥 - 如果您不想尝试 DeepSeek API，请跳到下一部分\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"你是一个乐于助人的助手\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many times does the letter 'a' appear in this sentence？（请用中文回复下面的问题）\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 DeepSeek Chat 回答一个更难的问题！并流式传输结果\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 DeepSeek Reasoner - 如果 DeepSeek 繁忙，可能会遇到错误\n",
    "# 如果失败，请过几天再来试试..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## 回到 OpenAI，问一个正经问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 言归正传！用 GPT-4o-mini 回答最初的问题\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个用 Markdown 格式回应的乐于助人的助手\"},\n",
    "    {\"role\": \"user\", \"content\": \"我如何判断一个业务问题是否适合用 LLM 解决方案？请用 Markdown 格式回应。\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让它以 markdown 格式流式返回结果\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## 现在来点有趣的 - 聊天机器人之间的对抗性对话...\n",
    "\n",
    "您已经熟悉了将提示词组织成如下列表的方式：\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "实际上，这种结构可以用来反映更长的对话历史：\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "我们可以使用这种方法来进行带有历史记录的更长时间的互动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让我们在 GPT-4o-mini 和 Claude-3-haiku 之间进行一次对话\n",
    "# 我们正在使用廉价版模型，所以成本会很低\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"你是一个非常喜欢抬杠的聊天机器人； \\\n",
    "你会反驳对话中的任何内容，并以尖刻的方式挑战一切。\"\n",
    "\n",
    "claude_system = \"你是一个非常有礼貌、谦恭的聊天机器人。你试图同意 \\\n",
    "对方所说的一切，或者寻找共同点。如果对方喜欢争论， \\\n",
    "你会试图让他们冷静下来并继续聊天。\"\n",
    "\n",
    "gpt_messages = [\"你好\"]\n",
    "claude_messages = [\"嗨\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"你好\"]\n",
    "claude_messages = [\"嗨\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">在您继续之前</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                请确保您理解了上面对话的工作原理，特别是 <code>messages</code> 列表是如何被填充的。根据需要添加打印语句。然后，为了获得一个很棒的变体，尝试使用系统提示来切换个性。也许一个可以变得悲观，另一个变得乐观？<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# 更高级的练习\n",
    "\n",
    "尝试创建一个三方对话，也许可以把 Gemini 也带入对话！\n",
    "\n",
    "在查看解决方案之前，请自己尝试一下。最简单的方法是使用 OpenAI python 客户端来访问 Gemini 模型（请参见上面的第二个 Gemini 示例）。\n",
    "\n",
    "## 附加练习\n",
    "\n",
    "您也可以尝试将其中一个模型替换为使用 Ollama 运行的开源模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">商业相关性</h2>\n",
    "            <span style=\"color:#181;\">这种以消息列表形式组织的对话结构，是我们构建对话式 AI 助手以及它们如何在对话中保持上下文的基础。我们将在接下来的几个实验中应用这一点来构建一个 AI 助手，然后您将把它扩展到您自己的业务中。</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
