


# 导入库

import os
import json
from dotenv import load_dotenv
from openai import OpenAI
import gradio as gr


# 初始化

load_dotenv(override=True)

openai_api_key = os.getenv('OPENAI_API_KEY')
if openai_api_key:
    print(f"OpenAI API 密钥存在，前缀为 {openai_api_key[:8]}")
else:
    print("未设置 OpenAI API 密钥")
    
MODEL = "gpt-4o-mini"
openai = OpenAI()


system_message = "你是一家名为 FlightAI 的航空公司的乐于助人的助手。 "
system_message += "请给出简短、礼貌的回答，不超过一句话。 "
system_message += "回答务必准确。如果不知道答案，请如实说明。"


# 这个函数看起来比我视频中的那个要简单得多，因为我们利用了最新的 Gradio 更新

def chat(message, history):
    messages = [{"role": "system", "content": system_message}] + history + [{"role": "user", "content": message}]
    response = openai.chat.completions.create(model=MODEL, messages=messages)
    return response.choices[0].message.content

gr.ChatInterface(fn=chat, type="messages").launch()





# 让我们从创建一个有用的函数开始

ticket_prices = {"london": "$799", "paris": "$899", "tokyo": "$1400", "berlin": "$499"}

def get_ticket_price(destination_city):
    print(f"工具 get_ticket_price 已为 {destination_city} 调用")
    city = destination_city.lower()
    return ticket_prices.get(city, "未知")


get_ticket_price("London")


# 描述我们的函数需要一个特定的字典结构：

price_function = {
    "name": "get_ticket_price",
    "description": "获取到目的地城市的往返机票价格。当您需要知道机票价格时（例如，当客户询问‘到这个城市的机票多少钱’时），请调用此函数",
    "parameters": {
        "type": "object",
        "properties": {
            "destination_city": {
                "type": "string",
                "description": "客户想要前往的城市",
            },
        },
        "required": ["destination_city"],
        "additionalProperties": False
    }
}


# 然后将其包含在一个工具列表中：

tools = [{"type": "function", "function": price_function}]





def chat(message, history):
    messages = [{"role": "system", "content": system_message}] + history + [{"role": "user", "content": message}]
    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)

    if response.choices[0].finish_reason=="tool_calls":
        message = response.choices[0].message
        response, city = handle_tool_call(message)
        messages.append(message)
        messages.append(response)
        response = openai.chat.completions.create(model=MODEL, messages=messages)
    
    return response.choices[0].message.content


# 我们必须编写 handle_tool_call 函数：

def handle_tool_call(message):
    tool_call = message.tool_calls[0]
    arguments = json.loads(tool_call.function.arguments)
    city = arguments.get('destination_city')
    price = get_ticket_price(city)
    response = {
        "role": "tool",
        "content": json.dumps({"destination_city": city,"price": price}),
        "tool_call_id": tool_call.id
    }
    return response, city


gr.ChatInterface(fn=chat, type="messages").launch()





# 用于处理图像的一些导入

import base64
from io import BytesIO
from PIL import Image


def artist(city):
    image_response = openai.images.generate(
            model="dall-e-3",
            prompt=f"一幅代表在 {city} 度假的照片，以充满活力的波普艺术风格展示旅游景点和 {city} 的一切独特之处",
            size="1024x1024",
            n=1,
            response_format="b64_json",
        )
    image_base64 = image_response.data[0].b64_json
    image_data = base64.b64decode(image_base64)
    return Image.open(BytesIO(image_data))


image = artist("New York City")
display(image)











!ffmpeg -version
!ffprobe -version
!ffplay -version





from pydub import AudioSegment
from pydub.playback import play

def talker(message):
    response = openai.audio.speech.create(
      model="tts-1",
      voice="onyx",    # 另外，可以尝试用 alloy 替换 onyx
      input=message
    )
    
    audio_stream = BytesIO(response.content)
    audio = AudioSegment.from_file(audio_stream, format="mp3")
    play(audio)


talker("你好呀")





import base64
from io import BytesIO
from PIL import Image
from IPython.display import Audio, display

def talker(message):
    response = openai.audio.speech.create(
        model="tts-1",
        voice="onyx",
        input=message)

    audio_stream = BytesIO(response.content)
    output_filename = "output_audio.mp3"
    with open(output_filename, "wb") as f:
        f.write(audio_stream.read())

    # 播放生成的音频
    display(Audio(output_filename, autoplay=True))

talker("你好呀")





import tempfile
import subprocess
from io import BytesIO
from pydub import AudioSegment
import time

def play_audio(audio_segment):
    temp_dir = tempfile.gettempdir()
    temp_path = os.path.join(temp_dir, "temp_audio.wav")
    try:
        audio_segment.export(temp_path, format="wav")
        time.sleep(3) # 学生 Dominic 发现这是必需的。您也可以尝试注释掉，看看在您的 PC 上是否需要
        subprocess.call([
            "ffplay",
            "-nodisp",
            "-autoexit",
            "-hide_banner",
            temp_path
        ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    finally:
        try:
            os.remove(temp_path)
        except Exception:
            pass
 
def talker(message):
    response = openai.audio.speech.create(
        model="tts-1",
        voice="onyx",  # 另外，可以尝试用 alloy 替换 onyx
        input=message
    )
    audio_stream = BytesIO(response.content)
    audio = AudioSegment.from_file(audio_stream, format="mp3")
    play_audio(audio)

talker("你好呀")





import os
from pydub import AudioSegment
from pydub.playback import play
from io import BytesIO

def talker(message):
    # 在 Windows 上为临时文件设置一个自定义目录
    custom_temp_dir = os.path.expanduser("~/Documents/temp_audio")
    os.environ['TEMP'] = custom_temp_dir  # 如果需要，您也可以使用 'TMP'
    
    # 如果文件夹不存在则创建
    if not os.path.exists(custom_temp_dir):
        os.makedirs(custom_temp_dir)
    
    response = openai.audio.speech.create(
        model="tts-1",
        voice="onyx",  # 另外，可以尝试用 alloy 替换 onyx
        input=message
    )
    
    audio_stream = BytesIO(response.content)
    audio = AudioSegment.from_file(audio_stream, format="mp3")

    play(audio)

talker("你好呀")





!pip install simpleaudio


from pydub import AudioSegment
from io import BytesIO
import tempfile
import os
import simpleaudio as sa

def talker(message):
    response = openai.audio.speech.create(
        model="tts-1",
        voice="onyx",  # 另外，可以尝试用 alloy 替换 onyx
        input=message
    )
    
    audio_stream = BytesIO(response.content)
    audio = AudioSegment.from_file(audio_stream, format="mp3")

    # 在您有写入权限的文件夹中创建一个临时文件
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False, dir=os.path.expanduser("~/Documents")) as temp_audio_file:
        temp_file_name = temp_audio_file.name
        audio.export(temp_file_name, format="wav")
    
    # 使用 simpleaudio 加载并播放音频
    wave_obj = sa.WaveObject.from_wave_file(temp_file_name)
    play_obj = wave_obj.play()
    play_obj.wait_done()  # 等待播放完成

    # 之后清理临时文件
    os.remove(temp_file_name)
    
talker("你好呀")








def chat(history):
    messages = [{"role": "system", "content": system_message}] + history
    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)
    image = None
    
    if response.choices[0].finish_reason=="tool_calls":
        message = response.choices[0].message
        response, city = handle_tool_call(message)
        messages.append(message)
        messages.append(response)
        image = artist(city)
        response = openai.chat.completions.create(model=MODEL, messages=messages)
        
    reply = response.choices[0].message.content
    history += [{"role":"assistant", "content":reply}]

    # 如果您暂时想跳过音频，请注释掉或删除下一行..
    talker(reply)
    
    return history, image


# 更复杂的 Gradio 代码，因为我们没有使用预设的聊天界面！
# 在最后一行传入 inbrowser=True 会立即弹出一个 Gradio 窗口。

with gr.Blocks() as ui:
    with gr.Row():
        chatbot = gr.Chatbot(height=500, type="messages")
        image_output = gr.Image(height=500)
    with gr.Row():
        entry = gr.Textbox(label="与我们的 AI 助手聊天：")
    with gr.Row():
        clear = gr.Button("清除")

    def do_entry(message, history):
        history += [{"role":"user", "content":message}]
        return "", history

    entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot]).then(
        chat, inputs=chatbot, outputs=[chatbot, image_output]
    )
    clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)

ui.launch(inbrowser=True)






